You are a question-answering assistant. Your task is to extract a short, precise span from the given context that answers the user's question. If no answer is found, reply "No answer in the context."

### Question:
‡∏™‡∏π‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏£‡∏ß‡∏°‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£

### Context:
‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πâ‡∏ô‡∏á‡∏ß‡∏î = 10 ‡∏ö‡∏≤‡∏ó
‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏•‡∏≤‡∏¢‡∏á‡∏ß‡∏î = 12 ‡∏ö‡∏≤‡∏ó
‡πÄ‡∏á‡∏¥‡∏ô‡∏õ‡∏±‡∏ô‡∏ú‡∏• = 1 ‡∏ö‡∏≤‡∏ó
‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏£‡∏ß‡∏° = (‡πÄ‡∏á‡∏¥‡∏ô‡∏õ‡∏±‡∏ô‡∏ú‡∏• + ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏•‡∏≤‡∏¢‡∏á‡∏ß‡∏î - ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πâ‡∏ô‡∏á‡∏ß‡∏î) / ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πâ‡∏ô‡∏á‡∏ß‡∏î

### Extracted Answer:


Model Ideas for Extractive LFQA
üí° Option 1: Fine-Tuned Transformers (Span Prediction)
Use pre-trained models like:
| Model | Highlights | Notes | 
| deepset/roberta-base-squad2 | Handles long-context & impossible questions | Well-suited for Thai-English mixed contexts if fine-tuned | 
| ThaiQA-BERT | Trained on Thai QA datasets | Great for native semantic structure | 
| LongformerQA | Supports longer documents natively | Use if context > 512 tokens | 


Key Benefit: These models predict start and end tokens from context, perfect for precise extraction.


from transformers import pipeline

# Load extractive QA model
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Example input
context = """
‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πâ‡∏ô‡∏á‡∏ß‡∏î = 10 ‡∏ö‡∏≤‡∏ó
‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏•‡∏≤‡∏¢‡∏á‡∏ß‡∏î = 12 ‡∏ö‡∏≤‡∏ó
‡πÄ‡∏á‡∏¥‡∏ô‡∏õ‡∏±‡∏ô‡∏ú‡∏• = 1 ‡∏ö‡∏≤‡∏ó
‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏£‡∏ß‡∏° = (‡πÄ‡∏á‡∏¥‡∏ô‡∏õ‡∏±‡∏ô‡∏ú‡∏• + ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏•‡∏≤‡∏¢‡∏á‡∏ß‡∏î - ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πâ‡∏ô‡∏á‡∏ß‡∏î) / ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πâ‡∏ô‡∏á‡∏ß‡∏î
"""

question = "‡∏™‡∏π‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏£‡∏ß‡∏°‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£"

# Get answer
result = qa_pipeline(question=question, context=context)

print(f"Answer: {result['answer']}")

